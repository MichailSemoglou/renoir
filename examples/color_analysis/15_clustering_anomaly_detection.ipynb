{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artwork Clustering & Anomaly Detection\n",
    "\n",
    "## Introduction\n",
    "\n",
    "So far we've used **supervised learning** where we tell the model what categories exist (Impressionism, Baroque, etc.). But what if we let the data speak for itself?\n",
    "\n",
    "In this lesson, we explore **unsupervised learning**—discovering natural groupings in art without predefined labels. We'll also find **anomalies**: artworks with unusual color usage that stand out from the crowd.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "1. **K-Means Clustering**: Partitioning artworks into color-based groups\n",
    "2. **Hierarchical Clustering**: Discovering nested relationships\n",
    "3. **DBSCAN**: Density-based clustering that finds outliers\n",
    "4. **Anomaly Detection**: Identifying unusual color usage\n",
    "5. **Similarity Search**: Finding artworks with similar palettes\n",
    "\n",
    "### Supervised vs Unsupervised\n",
    "\n",
    "| Approach | Labels | Goal | Example |\n",
    "|----------|--------|------|--------|\n",
    "| **Supervised** | Required | Predict known categories | \"Is this Impressionism?\" |\n",
    "| **Unsupervised** | Not needed | Discover natural structure | \"What groups exist?\" |\n",
    "\n",
    "Unsupervised learning can reveal patterns that art historians might not have considered—groupings based purely on visual properties rather than historical movements.\n",
    "\n",
    "Let's discover hidden structure in art!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Renoir imports\n",
    "from renoir import ArtistAnalyzer\n",
    "from renoir.color import ColorExtractor, ColorAnalyzer, ColorVisualizer\n",
    "\n",
    "# ML imports\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.neighbors import NearestNeighbors, LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize renoir components\n",
    "artist_analyzer = ArtistAnalyzer()\n",
    "color_extractor = ColorExtractor()\n",
    "color_analyzer = ColorAnalyzer()\n",
    "visualizer = ColorVisualizer()\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading WikiArt dataset...\")\n",
    "dataset = artist_analyzer._load_dataset()\n",
    "print(f\"Loaded {len(dataset)} artworks\")\n",
    "\n",
    "# Get metadata\n",
    "style_names = dataset.features['style'].names\n",
    "artist_names = dataset.features['artist'].names\n",
    "print(f\"Styles: {len(style_names)}, Artists: {len(artist_names)}\")\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building the Artwork Feature Dataset\n",
    "\n",
    "First, we need to extract color features from a diverse sample of artworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_artwork_features(image, n_colors=8):\n",
    "    \"\"\"\n",
    "    Extract comprehensive color features from an artwork.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        palette = color_extractor.extract_dominant_colors(image, n_colors=n_colors)\n",
    "        if not palette or len(palette) < 3:\n",
    "            return None\n",
    "        \n",
    "        stats = color_analyzer.analyze_palette_statistics(palette)\n",
    "        temp = color_analyzer.analyze_color_temperature_distribution(palette)\n",
    "        harmony = color_analyzer.analyze_color_harmony(palette)\n",
    "        \n",
    "        # HSV analysis\n",
    "        hsv_colors = [color_analyzer.rgb_to_hsv(c) for c in palette]\n",
    "        hues = [h[0] for h in hsv_colors]\n",
    "        sats = [h[1] for h in hsv_colors]\n",
    "        vals = [h[2] for h in hsv_colors]\n",
    "        \n",
    "        # RGB analysis\n",
    "        reds = [c[0] for c in palette]\n",
    "        greens = [c[1] for c in palette]\n",
    "        blues = [c[2] for c in palette]\n",
    "        \n",
    "        features = {\n",
    "            # Central tendencies\n",
    "            'mean_hue': stats['mean_hue'],\n",
    "            'mean_saturation': stats['mean_saturation'],\n",
    "            'mean_brightness': stats['mean_value'],\n",
    "            'mean_red': np.mean(reds),\n",
    "            'mean_green': np.mean(greens),\n",
    "            'mean_blue': np.mean(blues),\n",
    "            \n",
    "            # Variability\n",
    "            'std_hue': np.std(hues),\n",
    "            'std_saturation': np.std(sats),\n",
    "            'std_brightness': np.std(vals),\n",
    "            \n",
    "            # Temperature\n",
    "            'warm_ratio': temp['warm_percentage'] / 100,\n",
    "            'cool_ratio': temp['cool_percentage'] / 100,\n",
    "            'neutral_ratio': temp['neutral_percentage'] / 100,\n",
    "            \n",
    "            # Scores\n",
    "            'color_diversity': color_analyzer.calculate_color_diversity(palette),\n",
    "            'saturation_score': color_analyzer.calculate_saturation_score(palette),\n",
    "            'brightness_score': color_analyzer.calculate_brightness_score(palette),\n",
    "            \n",
    "            # Harmony\n",
    "            'harmony_score': harmony.get('harmony_score', 0),\n",
    "            \n",
    "            # Contrast\n",
    "            'brightness_range': max(vals) - min(vals),\n",
    "            'saturation_range': max(sats) - min(sats),\n",
    "        }\n",
    "        \n",
    "        return features, palette\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# Collect diverse artworks\n",
    "N_SAMPLES = 500  # Adjust based on time/resources\n",
    "\n",
    "print(f\"Collecting {N_SAMPLES} artworks...\")\n",
    "\n",
    "artworks_data = []\n",
    "artworks_meta = []\n",
    "artworks_palettes = []\n",
    "\n",
    "# Sample evenly across styles\n",
    "samples_per_style = max(10, N_SAMPLES // len(style_names))\n",
    "style_counts = defaultdict(int)\n",
    "\n",
    "for item in dataset:\n",
    "    style_idx = item['style']\n",
    "    \n",
    "    if style_counts[style_idx] >= samples_per_style:\n",
    "        continue\n",
    "    \n",
    "    result = extract_artwork_features(item['image'])\n",
    "    if result[0] is not None:\n",
    "        features, palette = result\n",
    "        artworks_data.append(features)\n",
    "        artworks_meta.append({\n",
    "            'style': style_names[style_idx],\n",
    "            'style_idx': style_idx,\n",
    "            'artist_idx': item['artist'],\n",
    "            'index': len(artworks_data) - 1\n",
    "        })\n",
    "        artworks_palettes.append(palette)\n",
    "        style_counts[style_idx] += 1\n",
    "    \n",
    "    if len(artworks_data) >= N_SAMPLES:\n",
    "        break\n",
    "    \n",
    "    if len(artworks_data) % 100 == 0:\n",
    "        print(f\"  Collected {len(artworks_data)} artworks...\")\n",
    "\n",
    "print(f\"\\nTotal artworks collected: {len(artworks_data)}\")\n",
    "print(f\"Styles represented: {len(style_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(artworks_data)\n",
    "meta_df = pd.DataFrame(artworks_meta)\n",
    "\n",
    "print(f\"Feature matrix shape: {df.shape}\")\n",
    "print(f\"\\nFeatures: {list(df.columns)}\")\n",
    "\n",
    "# Show style distribution\n",
    "print(f\"\\nStyle distribution:\")\n",
    "print(meta_df['style'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering\n",
    "X = df.values\n",
    "X = np.nan_to_num(X, nan=0.0)  # Handle any NaN values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Scaled feature matrix: {X_scaled.shape}\")\n",
    "print(f\"Feature mean (should be ~0): {X_scaled.mean():.6f}\")\n",
    "print(f\"Feature std (should be ~1): {X_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: K-Means Clustering\n",
    "\n",
    "K-Means partitions data into K clusters by minimizing within-cluster variance. But how do we choose K?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal K using elbow method and silhouette score\n",
    "K_range = range(2, 15)\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "\n",
    "print(\"Finding optimal number of clusters...\")\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=SEED, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    \n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouettes.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "    \n",
    "    print(f\"  K={k}: Inertia={kmeans.inertia_:.0f}, Silhouette={silhouettes[-1]:.3f}\")\n",
    "\n",
    "# Plot elbow and silhouette\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow plot\n",
    "axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[0].set_ylabel('Inertia (Within-cluster SS)', fontsize=12)\n",
    "axes[0].set_title('Elbow Method', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette plot\n",
    "axes[1].plot(K_range, silhouettes, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1].set_title('Silhouette Analysis', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Mark best silhouette\n",
    "best_k = K_range[np.argmax(silhouettes)]\n",
    "axes[1].axvline(x=best_k, color='red', linestyle='--', label=f'Best K={best_k}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle('Choosing Optimal Number of Clusters', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal K by silhouette: {best_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit K-Means with optimal K\n",
    "OPTIMAL_K = best_k\n",
    "\n",
    "kmeans = KMeans(n_clusters=OPTIMAL_K, random_state=SEED, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "print(f\"K-Means clustering with K={OPTIMAL_K}\")\n",
    "print(f\"\\nCluster sizes:\")\n",
    "for i in range(OPTIMAL_K):\n",
    "    count = (kmeans_labels == i).sum()\n",
    "    print(f\"  Cluster {i}: {count} artworks ({count/len(kmeans_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters with t-SNE\n",
    "print(\"Computing t-SNE projection...\")\n",
    "tsne = TSNE(n_components=2, random_state=SEED, perplexity=30, learning_rate='auto', init='pca')\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Color by K-Means cluster\n",
    "scatter1 = axes[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=kmeans_labels, \n",
    "                          cmap='tab10', s=50, alpha=0.6, edgecolors='white', linewidth=0.5)\n",
    "axes[0].set_title('K-Means Clusters (Unsupervised)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('t-SNE 1')\n",
    "axes[0].set_ylabel('t-SNE 2')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "\n",
    "# Color by actual art movement (for comparison)\n",
    "style_indices = meta_df['style_idx'].values\n",
    "scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=style_indices, \n",
    "                          cmap='tab20', s=50, alpha=0.6, edgecolors='white', linewidth=0.5)\n",
    "axes[1].set_title('Actual Art Movements (Ground Truth)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('t-SNE 1')\n",
    "axes[1].set_ylabel('t-SNE 2')\n",
    "\n",
    "plt.suptitle('K-Means Clusters vs Actual Art Movements', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster composition\n",
    "def analyze_cluster_composition(labels, meta_df):\n",
    "    \"\"\"\n",
    "    Analyze which art movements fall into each cluster.\n",
    "    \"\"\"\n",
    "    cluster_composition = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for label, meta in zip(labels, meta_df.to_dict('records')):\n",
    "        cluster_composition[label][meta['style']] += 1\n",
    "    \n",
    "    return cluster_composition\n",
    "\n",
    "\n",
    "composition = analyze_cluster_composition(kmeans_labels, meta_df)\n",
    "\n",
    "print(\"CLUSTER COMPOSITION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for cluster_id in sorted(composition.keys()):\n",
    "    styles = composition[cluster_id]\n",
    "    total = sum(styles.values())\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id} ({total} artworks):\")\n",
    "    \n",
    "    # Sort by count and show top styles\n",
    "    sorted_styles = sorted(styles.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for style, count in sorted_styles:\n",
    "        pct = count / total * 100\n",
    "        print(f\"  {style}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster profiles (what color features define each cluster?)\n",
    "def get_cluster_profiles(X, labels, feature_names):\n",
    "    \"\"\"\n",
    "    Compute mean feature values for each cluster.\n",
    "    \"\"\"\n",
    "    profiles = {}\n",
    "    for cluster_id in np.unique(labels):\n",
    "        mask = labels == cluster_id\n",
    "        cluster_mean = X[mask].mean(axis=0)\n",
    "        profiles[cluster_id] = dict(zip(feature_names, cluster_mean))\n",
    "    return profiles\n",
    "\n",
    "\n",
    "profiles = get_cluster_profiles(X, kmeans_labels, df.columns)\n",
    "\n",
    "# Create heatmap of cluster profiles\n",
    "profile_df = pd.DataFrame(profiles).T\n",
    "profile_df.index.name = 'Cluster'\n",
    "\n",
    "# Normalize for visualization\n",
    "profile_norm = (profile_df - profile_df.min()) / (profile_df.max() - profile_df.min())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.heatmap(profile_norm, annot=True, fmt='.2f', cmap='RdYlBu_r', \n",
    "            center=0.5, ax=ax, cbar_kws={'label': 'Normalized Value'})\n",
    "\n",
    "ax.set_title('K-Means Cluster Color Profiles', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Color Feature', fontsize=12)\n",
    "ax.set_ylabel('Cluster', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample palettes from each cluster\n",
    "def show_cluster_palettes(cluster_id, labels, palettes, n_samples=5):\n",
    "    \"\"\"\n",
    "    Display sample palettes from a cluster.\n",
    "    \"\"\"\n",
    "    mask = labels == cluster_id\n",
    "    indices = np.where(mask)[0]\n",
    "    \n",
    "    if len(indices) == 0:\n",
    "        return\n",
    "    \n",
    "    sample_indices = np.random.choice(indices, min(n_samples, len(indices)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(sample_indices), figsize=(3*len(sample_indices), 2))\n",
    "    if len(sample_indices) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, idx in zip(axes, sample_indices):\n",
    "        palette = palettes[idx]\n",
    "        for j, color in enumerate(palette[:5]):\n",
    "            color_norm = tuple(c/255 for c in color)\n",
    "            ax.add_patch(plt.Rectangle((j, 0), 1, 1, facecolor=color_norm, edgecolor='white', lw=0.5))\n",
    "        ax.set_xlim(0, 5)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Cluster {cluster_id} - Sample Palettes', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Show palettes for each cluster\n",
    "for cluster_id in range(OPTIMAL_K):\n",
    "    show_cluster_palettes(cluster_id, kmeans_labels, artworks_palettes, n_samples=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering builds a tree of clusters, revealing nested relationships at multiple scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute linkage matrix\n",
    "print(\"Computing hierarchical clustering...\")\n",
    "\n",
    "# Use a subset for clearer dendrogram\n",
    "sample_size = min(200, len(X_scaled))\n",
    "sample_indices = np.random.choice(len(X_scaled), sample_size, replace=False)\n",
    "X_sample = X_scaled[sample_indices]\n",
    "meta_sample = meta_df.iloc[sample_indices]\n",
    "\n",
    "# Compute linkage\n",
    "linkage_matrix = linkage(X_sample, method='ward')\n",
    "\n",
    "print(f\"Linkage matrix computed for {sample_size} artworks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dendrogram\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "# Create labels from style names\n",
    "labels = [s[:15] for s in meta_sample['style'].values]  # Truncate long names\n",
    "\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    labels=labels,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=6,\n",
    "    ax=ax,\n",
    "    color_threshold=0.5 * max(linkage_matrix[:, 2])\n",
    ")\n",
    "\n",
    "ax.set_title('Hierarchical Clustering of Artworks by Color', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Artwork (by Art Movement)', fontsize=12)\n",
    "ax.set_ylabel('Distance', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut dendrogram at different levels\n",
    "def analyze_hierarchical_levels(linkage_matrix, meta_df, n_clusters_list=[3, 5, 8]):\n",
    "    \"\"\"\n",
    "    Analyze cluster composition at different hierarchy levels.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(n_clusters_list), figsize=(6*len(n_clusters_list), 5))\n",
    "    \n",
    "    for ax, n_clusters in zip(axes, n_clusters_list):\n",
    "        # Cut dendrogram\n",
    "        labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')\n",
    "        \n",
    "        # Count styles per cluster\n",
    "        cluster_styles = defaultdict(list)\n",
    "        for label, style in zip(labels, meta_df['style'].values):\n",
    "            cluster_styles[label].append(style)\n",
    "        \n",
    "        # Create composition matrix\n",
    "        all_styles = list(set(meta_df['style']))\n",
    "        composition = np.zeros((n_clusters, len(all_styles)))\n",
    "        \n",
    "        for cluster_id, styles in cluster_styles.items():\n",
    "            style_counts = Counter(styles)\n",
    "            for style, count in style_counts.items():\n",
    "                style_idx = all_styles.index(style)\n",
    "                composition[cluster_id-1, style_idx] = count\n",
    "        \n",
    "        # Normalize rows\n",
    "        composition = composition / composition.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        # Plot\n",
    "        im = ax.imshow(composition, cmap='Blues', aspect='auto')\n",
    "        ax.set_title(f'{n_clusters} Clusters', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Art Movement')\n",
    "        ax.set_ylabel('Cluster')\n",
    "        ax.set_yticks(range(n_clusters))\n",
    "        ax.set_yticklabels([f'C{i+1}' for i in range(n_clusters)])\n",
    "    \n",
    "    plt.suptitle('Hierarchical Clustering: Style Composition at Different Levels', \n",
    "                 fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "analyze_hierarchical_levels(linkage_matrix, meta_sample, n_clusters_list=[3, 5, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative clustering on full dataset\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=OPTIMAL_K, linkage='ward')\n",
    "agg_labels = agg_clustering.fit_predict(X_scaled)\n",
    "\n",
    "print(f\"Agglomerative clustering with {OPTIMAL_K} clusters\")\n",
    "print(f\"Silhouette score: {silhouette_score(X_scaled, agg_labels):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: DBSCAN - Finding Natural Clusters and Outliers\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering) finds clusters of arbitrary shape and automatically identifies outliers as points in low-density regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal epsilon using k-distance graph\n",
    "k = 5  # Number of neighbors\n",
    "nn = NearestNeighbors(n_neighbors=k)\n",
    "nn.fit(X_scaled)\n",
    "distances, indices = nn.kneighbors(X_scaled)\n",
    "\n",
    "# Sort distances to k-th nearest neighbor\n",
    "k_distances = np.sort(distances[:, k-1])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(k_distances, linewidth=2)\n",
    "ax.set_xlabel('Points (sorted by distance)', fontsize=12)\n",
    "ax.set_ylabel(f'{k}-th Nearest Neighbor Distance', fontsize=12)\n",
    "ax.set_title('K-Distance Graph for DBSCAN Epsilon Selection', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Find elbow (approximate)\n",
    "elbow_idx = int(len(k_distances) * 0.9)  # 90th percentile\n",
    "suggested_eps = k_distances[elbow_idx]\n",
    "ax.axhline(y=suggested_eps, color='red', linestyle='--', label=f'Suggested ε={suggested_eps:.2f}')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Suggested epsilon: {suggested_eps:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DBSCAN\n",
    "EPS = suggested_eps\n",
    "MIN_SAMPLES = 5\n",
    "\n",
    "dbscan = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES)\n",
    "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_outliers = (dbscan_labels == -1).sum()\n",
    "\n",
    "print(f\"DBSCAN Results:\")\n",
    "print(f\"  Clusters found: {n_clusters}\")\n",
    "print(f\"  Outliers (noise): {n_outliers} ({n_outliers/len(dbscan_labels)*100:.1f}%)\")\n",
    "\n",
    "# Cluster sizes\n",
    "print(f\"\\nCluster sizes:\")\n",
    "for label in sorted(set(dbscan_labels)):\n",
    "    if label == -1:\n",
    "        name = \"Outliers\"\n",
    "    else:\n",
    "        name = f\"Cluster {label}\"\n",
    "    count = (dbscan_labels == label).sum()\n",
    "    print(f\"  {name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DBSCAN results\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Plot clusters\n",
    "unique_labels = set(dbscan_labels)\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "for label, color in zip(sorted(unique_labels), colors):\n",
    "    mask = dbscan_labels == label\n",
    "    \n",
    "    if label == -1:\n",
    "        # Outliers in black\n",
    "        ax.scatter(X_tsne[mask, 0], X_tsne[mask, 1], c='black', \n",
    "                  s=100, alpha=0.8, marker='x', linewidths=2, label='Outliers')\n",
    "    else:\n",
    "        ax.scatter(X_tsne[mask, 0], X_tsne[mask, 1], c=[color], \n",
    "                  s=50, alpha=0.6, edgecolors='white', linewidth=0.5, label=f'Cluster {label}')\n",
    "\n",
    "ax.set_title('DBSCAN Clustering with Outliers', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('t-SNE 1', fontsize=12)\n",
    "ax.set_ylabel('t-SNE 2', fontsize=12)\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Anomaly Detection\n",
    "\n",
    "Beyond DBSCAN's outliers, let's use dedicated anomaly detection algorithms to find artworks with unusual color usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest\n",
    "print(\"Running Isolation Forest...\")\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=SEED, n_estimators=100)\n",
    "iso_predictions = iso_forest.fit_predict(X_scaled)\n",
    "iso_scores = iso_forest.decision_function(X_scaled)\n",
    "\n",
    "n_anomalies_iso = (iso_predictions == -1).sum()\n",
    "print(f\"Isolation Forest anomalies: {n_anomalies_iso}\")\n",
    "\n",
    "# Local Outlier Factor\n",
    "print(\"\\nRunning Local Outlier Factor...\")\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "lof_predictions = lof.fit_predict(X_scaled)\n",
    "lof_scores = -lof.negative_outlier_factor_  # Higher = more anomalous\n",
    "\n",
    "n_anomalies_lof = (lof_predictions == -1).sum()\n",
    "print(f\"LOF anomalies: {n_anomalies_lof}\")\n",
    "\n",
    "# Combine: artworks flagged by both methods\n",
    "combined_anomalies = (iso_predictions == -1) & (lof_predictions == -1)\n",
    "n_combined = combined_anomalies.sum()\n",
    "print(f\"\\nAnomalies detected by BOTH methods: {n_combined}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize anomaly scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Isolation Forest\n",
    "scatter1 = axes[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=iso_scores, \n",
    "                          cmap='RdYlBu', s=50, alpha=0.7)\n",
    "axes[0].scatter(X_tsne[iso_predictions == -1, 0], X_tsne[iso_predictions == -1, 1],\n",
    "               facecolors='none', edgecolors='red', s=150, linewidth=2, label='Anomalies')\n",
    "axes[0].set_title('Isolation Forest Anomaly Scores', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Anomaly Score')\n",
    "\n",
    "# Local Outlier Factor\n",
    "scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=lof_scores, \n",
    "                          cmap='RdYlBu_r', s=50, alpha=0.7)\n",
    "axes[1].scatter(X_tsne[lof_predictions == -1, 0], X_tsne[lof_predictions == -1, 1],\n",
    "               facecolors='none', edgecolors='red', s=150, linewidth=2, label='Anomalies')\n",
    "axes[1].set_title('Local Outlier Factor Scores', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "plt.colorbar(scatter2, ax=axes[1], label='LOF Score')\n",
    "\n",
    "plt.suptitle('Anomaly Detection Results', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the most anomalous artworks\n",
    "def analyze_anomalies(anomaly_mask, X, df, meta_df, palettes, top_n=10):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the most anomalous artworks.\n",
    "    \"\"\"\n",
    "    anomaly_indices = np.where(anomaly_mask)[0]\n",
    "    \n",
    "    if len(anomaly_indices) == 0:\n",
    "        print(\"No anomalies found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nANOMALOUS ARTWORKS ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get global feature statistics for comparison\n",
    "    global_means = df.mean()\n",
    "    global_stds = df.std()\n",
    "    \n",
    "    for i, idx in enumerate(anomaly_indices[:top_n]):\n",
    "        meta = meta_df.iloc[idx]\n",
    "        features = df.iloc[idx]\n",
    "        palette = palettes[idx]\n",
    "        \n",
    "        print(f\"\\nAnomaly {i+1}: {meta['style']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Find features that deviate most from norm\n",
    "        z_scores = (features - global_means) / global_stds\n",
    "        extreme_features = z_scores.abs().sort_values(ascending=False).head(3)\n",
    "        \n",
    "        print(\"Unusual features (z-score):\")\n",
    "        for feat, z in extreme_features.items():\n",
    "            direction = \"HIGH\" if z_scores[feat] > 0 else \"LOW\"\n",
    "            print(f\"  {feat}: {direction} (z={z_scores[feat]:.2f})\")\n",
    "    \n",
    "    # Visualize anomalous palettes\n",
    "    n_show = min(8, len(anomaly_indices))\n",
    "    fig, axes = plt.subplots(2, n_show//2, figsize=(3*n_show//2, 4))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for ax, idx in zip(axes, anomaly_indices[:n_show]):\n",
    "        palette = palettes[idx]\n",
    "        style = meta_df.iloc[idx]['style']\n",
    "        \n",
    "        for j, color in enumerate(palette[:5]):\n",
    "            color_norm = tuple(c/255 for c in color)\n",
    "            ax.add_patch(plt.Rectangle((j, 0), 1, 1, facecolor=color_norm, edgecolor='white', lw=0.5))\n",
    "        \n",
    "        ax.set_xlim(0, 5)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(style[:20], fontsize=9)\n",
    "    \n",
    "    plt.suptitle('Anomalous Artwork Palettes', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Analyze anomalies detected by both methods\n",
    "analyze_anomalies(combined_anomalies, X, df, meta_df, artworks_palettes, top_n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Color-Based Similarity Search\n",
    "\n",
    "Now let's build a system to find artworks with similar color palettes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorSimilaritySearch:\n",
    "    \"\"\"\n",
    "    Find artworks with similar color features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X_scaled, meta_df, palettes, n_neighbors=10):\n",
    "        self.X = X_scaled\n",
    "        self.meta_df = meta_df\n",
    "        self.palettes = palettes\n",
    "        \n",
    "        # Build nearest neighbors index\n",
    "        self.nn = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine')\n",
    "        self.nn.fit(X_scaled)\n",
    "    \n",
    "    def search(self, query_idx, n_results=5):\n",
    "        \"\"\"\n",
    "        Find similar artworks to the query.\n",
    "        \"\"\"\n",
    "        query = self.X[query_idx].reshape(1, -1)\n",
    "        distances, indices = self.nn.kneighbors(query, n_neighbors=n_results+1)\n",
    "        \n",
    "        # Skip first result (the query itself)\n",
    "        results = []\n",
    "        for dist, idx in zip(distances[0][1:], indices[0][1:]):\n",
    "            results.append({\n",
    "                'index': idx,\n",
    "                'distance': dist,\n",
    "                'similarity': 1 - dist,\n",
    "                'style': self.meta_df.iloc[idx]['style'],\n",
    "                'palette': self.palettes[idx]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def search_by_features(self, features_dict, n_results=5):\n",
    "        \"\"\"\n",
    "        Search by specifying desired color features.\n",
    "        \"\"\"\n",
    "        # This would require access to the scaler - simplified version\n",
    "        pass\n",
    "    \n",
    "    def visualize_search(self, query_idx, n_results=5):\n",
    "        \"\"\"\n",
    "        Visualize search results.\n",
    "        \"\"\"\n",
    "        results = self.search(query_idx, n_results)\n",
    "        \n",
    "        # Get query info\n",
    "        query_style = self.meta_df.iloc[query_idx]['style']\n",
    "        query_palette = self.palettes[query_idx]\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(2, n_results + 1, figsize=(3*(n_results+1), 4))\n",
    "        \n",
    "        # Query (first column)\n",
    "        ax = axes[0, 0]\n",
    "        ax.text(0.5, 0.5, 'QUERY', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        ax = axes[1, 0]\n",
    "        for j, color in enumerate(query_palette[:5]):\n",
    "            color_norm = tuple(c/255 for c in color)\n",
    "            ax.add_patch(plt.Rectangle((j, 0), 1, 1, facecolor=color_norm, edgecolor='white', lw=0.5))\n",
    "        ax.set_xlim(0, 5)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(query_style[:20], fontsize=9)\n",
    "        \n",
    "        # Results\n",
    "        for i, result in enumerate(results):\n",
    "            # Similarity score\n",
    "            ax = axes[0, i+1]\n",
    "            ax.text(0.5, 0.5, f'{result[\"similarity\"]:.0%}', \n",
    "                   ha='center', va='center', fontsize=14, fontweight='bold',\n",
    "                   color='green' if result['similarity'] > 0.8 else 'orange')\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Palette\n",
    "            ax = axes[1, i+1]\n",
    "            for j, color in enumerate(result['palette'][:5]):\n",
    "                color_norm = tuple(c/255 for c in color)\n",
    "                ax.add_patch(plt.Rectangle((j, 0), 1, 1, facecolor=color_norm, edgecolor='white', lw=0.5))\n",
    "            ax.set_xlim(0, 5)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(result['style'][:20], fontsize=9)\n",
    "        \n",
    "        plt.suptitle('Color Similarity Search Results', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Create search engine\n",
    "search_engine = ColorSimilaritySearch(X_scaled, meta_df, artworks_palettes)\n",
    "\n",
    "print(\"Color similarity search engine ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Search for similar artworks\n",
    "# Pick a random query\n",
    "query_idx = np.random.randint(0, len(X_scaled))\n",
    "\n",
    "print(f\"Query artwork: {meta_df.iloc[query_idx]['style']}\")\n",
    "results = search_engine.visualize_search(query_idx, n_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More search examples\n",
    "# Find an Impressionist and search for similar\n",
    "impressionist_idx = meta_df[meta_df['style'] == 'Impressionism'].index[0]\n",
    "print(f\"\\nSearching for artworks similar to: Impressionism\")\n",
    "results = search_engine.visualize_search(impressionist_idx, n_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search from a Baroque artwork\n",
    "baroque_indices = meta_df[meta_df['style'] == 'Baroque'].index\n",
    "if len(baroque_indices) > 0:\n",
    "    baroque_idx = baroque_indices[0]\n",
    "    print(f\"\\nSearching for artworks similar to: Baroque\")\n",
    "    results = search_engine.visualize_search(baroque_idx, n_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Cluster Quality Analysis\n",
    "\n",
    "Let's compare the different clustering methods and evaluate their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare clustering methods\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "# Ground truth: style labels\n",
    "true_labels = meta_df['style_idx'].values\n",
    "\n",
    "# Clustering results\n",
    "clustering_results = {\n",
    "    'K-Means': kmeans_labels,\n",
    "    'Agglomerative': agg_labels,\n",
    "    'DBSCAN': dbscan_labels\n",
    "}\n",
    "\n",
    "print(\"CLUSTERING QUALITY COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Method':<20} {'Silhouette':<15} {'ARI':<15} {'NMI':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for name, labels in clustering_results.items():\n",
    "    # Filter out noise points for DBSCAN\n",
    "    if name == 'DBSCAN':\n",
    "        mask = labels != -1\n",
    "        if mask.sum() < 2:\n",
    "            print(f\"{name:<20} {'N/A':<15} {'N/A':<15} {'N/A':<15}\")\n",
    "            continue\n",
    "        sil = silhouette_score(X_scaled[mask], labels[mask])\n",
    "        ari = adjusted_rand_score(true_labels[mask], labels[mask])\n",
    "        nmi = normalized_mutual_info_score(true_labels[mask], labels[mask])\n",
    "    else:\n",
    "        sil = silhouette_score(X_scaled, labels)\n",
    "        ari = adjusted_rand_score(true_labels, labels)\n",
    "        nmi = normalized_mutual_info_score(true_labels, labels)\n",
    "    \n",
    "    print(f\"{name:<20} {sil:<15.3f} {ari:<15.3f} {nmi:<15.3f}\")\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "print(\"  Silhouette: Cluster cohesion/separation (-1 to 1, higher is better)\")\n",
    "print(\"  ARI: Agreement with true labels (-1 to 1, higher is better)\")\n",
    "print(\"  NMI: Mutual information with true labels (0 to 1, higher is better)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette analysis per cluster\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "sample_silhouette_values = silhouette_samples(X_scaled, kmeans_labels)\n",
    "\n",
    "y_lower = 10\n",
    "for i in range(OPTIMAL_K):\n",
    "    cluster_silhouette_values = sample_silhouette_values[kmeans_labels == i]\n",
    "    cluster_silhouette_values.sort()\n",
    "    \n",
    "    cluster_size = len(cluster_silhouette_values)\n",
    "    y_upper = y_lower + cluster_size\n",
    "    \n",
    "    color = plt.cm.Set1(i / OPTIMAL_K)\n",
    "    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_values,\n",
    "                     facecolor=color, edgecolor=color, alpha=0.7)\n",
    "    \n",
    "    ax.text(-0.05, y_lower + 0.5 * cluster_size, str(i), fontsize=10, fontweight='bold')\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "ax.axvline(x=silhouette_score(X_scaled, kmeans_labels), color='red', linestyle='--',\n",
    "           label=f'Average: {silhouette_score(X_scaled, kmeans_labels):.3f}')\n",
    "\n",
    "ax.set_xlabel('Silhouette Coefficient', fontsize=12)\n",
    "ax.set_ylabel('Cluster', fontsize=12)\n",
    "ax.set_title('Silhouette Analysis for K-Means Clustering', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Insights and Interpretation\n",
    "\n",
    "### What We Discovered\n",
    "\n",
    "1. **Natural Color Groupings**: Artworks cluster by color properties that don't always align with art historical categories\n",
    "\n",
    "2. **Cross-Movement Similarities**: Some artworks from different movements share color DNA\n",
    "\n",
    "3. **Anomalies**: Unusual color usage exists across all movements - artists who broke color conventions\n",
    "\n",
    "4. **Hierarchy**: Color relationships exist at multiple scales (broad groupings → fine distinctions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Total artworks analyzed: {len(X_scaled)}\")\n",
    "print(f\"  Features per artwork: {X_scaled.shape[1]}\")\n",
    "print(f\"  Art movements represented: {meta_df['style'].nunique()}\")\n",
    "\n",
    "print(f\"\\nClustering:\")\n",
    "print(f\"  Optimal K (by silhouette): {OPTIMAL_K}\")\n",
    "print(f\"  K-Means silhouette score: {silhouette_score(X_scaled, kmeans_labels):.3f}\")\n",
    "\n",
    "print(f\"\\nAnomaly Detection:\")\n",
    "print(f\"  Isolation Forest anomalies: {n_anomalies_iso}\")\n",
    "print(f\"  LOF anomalies: {n_anomalies_lof}\")\n",
    "print(f\"  Consensus anomalies: {n_combined}\")\n",
    "\n",
    "print(f\"\\nDBSCAN:\")\n",
    "print(f\"  Clusters found: {n_clusters}\")\n",
    "print(f\"  Noise points: {n_outliers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Exercise 1: Try different clustering algorithms\n",
    "# - Spectral Clustering\n",
    "# - Gaussian Mixture Models\n",
    "# - Mean Shift\n",
    "\n",
    "# Exercise 2: Feature engineering\n",
    "# Add histogram-based features or texture features\n",
    "# See how clustering changes\n",
    "\n",
    "# Exercise 3: Cluster interpretation\n",
    "# For each cluster, generate a \"representative palette\"\n",
    "# by averaging the palettes in that cluster\n",
    "\n",
    "# Exercise 4: Anomaly deep dive\n",
    "# Investigate the anomalies - are they truly unusual\n",
    "# or just from underrepresented styles?\n",
    "\n",
    "# Exercise 5: Build a recommendation system\n",
    "# \"If you like this artwork, you might also like...\"\n",
    "# using the similarity search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lesson, you've explored **unsupervised learning** for art analysis:\n",
    "\n",
    "1. **K-Means Clustering**: Partitioned artworks into color-based groups\n",
    "2. **Hierarchical Clustering**: Discovered nested relationships and natural groupings\n",
    "3. **DBSCAN**: Found density-based clusters and identified outliers\n",
    "4. **Anomaly Detection**: Used Isolation Forest and LOF to find unusual color usage\n",
    "5. **Similarity Search**: Built a system to find artworks with similar palettes\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "Unsupervised learning reveals structure that **doesn't depend on predefined categories**. The clusters we found are based purely on color properties—they might align with art historical movements, but they might also reveal **new ways of thinking about artistic similarity** based on visual properties alone.\n",
    "\n",
    "This is the power of letting the data speak for itself: we can discover patterns that art historians might not have considered, identify outliers who broke color conventions, and build systems that find unexpected connections between artworks across time and style."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
